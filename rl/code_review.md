## ä¸€ã€æ•´ä½“æ¶æ„æ€»è§ˆï¼ˆå¼‚æ­¥åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼‰

### 1.1 ç³»ç»Ÿç»„ä»¶è§’è‰²

åœ¨è¿ç»­ç‰ˆå’Œç¦»æ•£ç‰ˆä¸¤ä¸ªè„šæœ¬ä¸­ï¼Œæ•´ä½“æ¶æ„åŸºæœ¬ä¸€è‡´ï¼Œä»…åœ¨åŠ¨ä½œç©ºé—´å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šå­˜åœ¨å·®å¼‚ã€‚ä¸‹é¢ä»‹ç»å„æ ¸å¿ƒç»„ä»¶ï¼š

#### **TrainerActor**ï¼ˆè®­ç»ƒå™¨ï¼ŒRay Actor + DeepSpeedï¼‰
- å¤š GPU ä¸Šçš„ç­–ç•¥/ä»·å€¼ç½‘ç»œè®­ç»ƒæ ¸å¿ƒ
- ä½¿ç”¨ DeepSpeed ZeRO-2 è¿›è¡Œæ•°æ®å¹¶è¡Œä¸é€šä¿¡ä¼˜åŒ–
- é€šè¿‡å¼‚æ­¥åå°åç¨‹ `_data_fetching_loop` æŒç»­ä» ReplayBuffer æ‹‰å–å¤§æ‰¹é‡æ•°æ®ï¼ˆè¶…çº§æ‰¹æ¬¡ï¼‰ï¼Œä¸å‰å° PPO æ›´æ–°å¾ªç¯å®Œå…¨è§£è€¦

#### **InferenceActor**ï¼ˆæ¨ç†å™¨ï¼ŒRay Actor + å•/å¤š GPUï¼‰
- å¸¸é©»åœ¨æŒ‡å®š GPU ä¸Šï¼Œä»…è´Ÿè´£å‰å‘æ¨ç†ï¼Œä¸å‚ä¸åå‘ä¼ æ’­
- æŒæœ‰ ActorCritic æ¨¡å‹çš„ä¸€ä»½å‰¯æœ¬
- æ¥æ”¶æ¥è‡ªå¤šä¸ª RolloutWorker çš„æ¨ç†è¯·æ±‚ï¼Œé€šè¿‡å¼‚æ­¥é˜Ÿåˆ—ä¸æ‰¹å¤„ç†å®ç°é«˜ååæ¨ç†æœåŠ¡

#### **RolloutWorkerActor**ï¼ˆé‡‡æ · Workerï¼‰
- ä¸åŠ è½½å®Œæ•´å¤§æ¨¡å‹ï¼Œä»…æŒæœ‰ Processor å’Œç¯å¢ƒ
- è°ƒç”¨ InferenceActor è·å–åŠ¨ä½œ/logits/valueï¼Œä¸ç¯å¢ƒäº¤äº’ã€ç§¯ç´¯è½¨è¿¹
- åœ¨æœ¬åœ°è®¡ç®— GAEï¼ˆå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼‰ä¸å›æŠ¥ï¼Œç„¶åæ‰“åŒ…æˆ Experience å†™å…¥ ReplayBuffer

#### **ReplayBufferActor**ï¼ˆç»éªŒæ± ï¼‰
- è½»é‡çš„è¿œç¨‹ FIFO/éšæœºé‡‡æ ·ç¼“å†²åŒº
- æ¥æ”¶æ¥è‡ªä¼—å¤š RolloutWorkers çš„ `add_batch()` è°ƒç”¨
- ä¸º TrainerActor æä¾› `sample(super_batch_size)` é‡‡æ ·æ¥å£

#### **StatsActor / EvaluationWorkerActor**
- **StatsActor**ï¼šèšåˆå„ç¯å¢ƒçš„å¹³å‡å›æŠ¥ã€è½¨è¿¹é•¿åº¦ã€æˆåŠŸç‡ç­‰å…³é”®ç»Ÿè®¡ä¿¡æ¯
- **EvaluationWorkerActor**ï¼ˆä»…åœ¨ç¦»æ•£ç‰ˆä¸­ï¼‰ï¼šä½¿ç”¨å½“å‰ç­–ç•¥è¿›è¡Œè¯„ä¼° Rolloutï¼Œä¸å‘ ReplayBuffer å†™æ•°æ®ï¼Œç”¨äºåœ¨çº¿æ€§èƒ½è¯„ä¼°

#### **ds_com é€šä¿¡æ¨¡å—**
- `TrainerActorCom` / `InferenceActorCom` æŠ½è±¡äº†è®­ç»ƒå™¨ â†’ æ¨ç†å™¨çš„æƒé‡å¹¿æ’­æœºåˆ¶
- å°è£… `torch.distributed` è¿›ç¨‹ç»„åˆå§‹åŒ–ä¸ Broadcast æ“ä½œ
- æ”¯æŒ ZeRO-2 ä¸‹çš„å‚æ•°èšåˆåå†å¹¿æ’­

### 1.2 ç³»ç»Ÿæ‹“æ‰‘ä¸æ•°æ®æµï¼ˆé€»è¾‘æ—¶åºï¼‰

æ•´ä½“æ‹“æ‰‘å¯ä»¥æ¦‚æ‹¬å¦‚ä¸‹ï¼š

1. **TrainerActor** åœ¨å¤š GPU ä¸Šç”¨ DeepSpeed åˆå§‹åŒ–è®­ç»ƒè¿›ç¨‹ç»„
2. **InferenceActor** åœ¨ä¸€å—ï¼ˆæˆ–å‡ å—ï¼‰ç‹¬ç«‹ GPU ä¸ŠåŠ è½½åŒæ¬¾ ActorCritic æ¨¡å‹
3. **TrainerActor** å‘¨æœŸæ€§å°†æœ€æ–°æƒé‡é€šè¿‡ `TrainerActorCom.broadcast_weights` â†’ NCCL ç»„ â†’ `InferenceActorCom` æ¥æ”¶å¹¶æ›´æ–°
4. å¤§é‡ **RolloutWorkerActor** æŒç»­è°ƒç”¨ InferenceActor çš„å¼‚æ­¥æ¨ç†æ¥å£ï¼Œä»ç¯å¢ƒé‡‡æ ·ï¼Œå†™å…¥ ReplayBuffer
5. **TrainerActor** åå°åç¨‹ä» ReplayBuffer æ‹‰å–è¶…çº§æ‰¹æ¬¡ï¼Œå‰å°åç¨‹æ‰§è¡Œå¤šæ­¥æ¢¯åº¦æ›´æ–°
6. **StatsActor / EvaluationWorkerActor** å¼‚æ­¥è®°å½•æŒ‡æ ‡ã€è·‘è¯„ä¼°

**æ ¸å¿ƒäº®ç‚¹**ï¼šRolloutã€æ¨ç†ã€è®­ç»ƒä¸‰è€…é€šè¿‡ Ray + asyncio è§£è€¦ï¼Œåœ¨æ—¶é—´è½´ä¸Šé«˜åº¦é‡å ï¼Œå®ç°äº† GPU çš„é«˜åˆ©ç”¨ç‡å’Œå…¸å‹ "å¼‚æ­¥å¼" åˆ†å¸ƒè®­ç»ƒèŒƒå¼ã€‚

---



åœ¨è„šæœ¬å¼€å¤´å®šä¹‰äº†ä¸€ç»„å…³é”®è¶…å‚æ•°ï¼š

NUM_TRAINER_GPUS = 4
NUM_INFERENCE_ACTORS = 1
NUM_ROLLOUT_WORKERS = 40
ROLLOUT_LOCAL_BUF = 64
INFERENCE_BATCH = 8
INFERENCE_TIMEOUT_MS = 300
REPLAY_CAPACITY = 1000
TRAIN_BATCH_SIZE = 20
ACCUMULATION_STEPS = 13
SUPER_BATCH_SIZE = 260
LOG_INTERVAL_SECONDS = 10


å«ä¹‰å’Œèµ„æºåˆ©ç”¨æ„å›¾ï¼š

4 å¼ è®­ç»ƒ GPU + 1 ä¸ªæ¨ç† Actor + 40 ä¸ª rollout workerï¼š

è®­ç»ƒ GPU é€šè¿‡ DeepSpeed ZeRO-2 è¿›è¡Œ æ•°æ®å¹¶è¡Œã€‚

æ¨ç†ä¸è®­ç»ƒç‰©ç†ä¸Šå¯ä»¥åˆ† GPUï¼ˆä¾‹å¦‚ CUDA_VISIBLE_DEVICES="3,4,5,6,7"ï¼Œå…¶ä¸­ 3â€“6 è®­ç»ƒï¼Œ7 æ¨ç†ï¼‰ï¼Œé¿å…æ¨ç†/è®­ç»ƒæŠ¢æ˜¾å­˜ã€‚

40 ä¸ª rollout worker å……åˆ†æ‰“æ»¡ InferenceActor çš„è¯·æ±‚é˜Ÿåˆ—ï¼Œéšè—æ¨ç†/ç¯å¢ƒå»¶è¿Ÿã€‚

INFERENCE_BATCH + TIMEOUT æœºåˆ¶ï¼š

InferenceActor ä»¥ INFERENCE_BATCH=8 ä¸ºå•ä½åš batched forwardã€‚

è‹¥çŸ­æ—¶é—´å†…è¯·æ±‚ä¸è¶³ batch sizeï¼ŒINFERENCE_TIMEOUT_MS åˆ°æœŸå°±ç”¨å°æ‰¹æ¬¡å¼ºåˆ¶æ¨ç†ï¼Œé¿å…ç­‰å¾…è¿‡ä¹…ã€‚

SUPER_BATCH_SIZE / TRAIN_BATCH_SIZE / ACCUMULATION_STEPSï¼š

Trainer æ¯æ¬¡ä» ReplayBuffer å–ä¸€ä¸ª è¶…çº§æ‰¹æ¬¡ï¼ˆ260 è½¨è¿¹æ ·æœ¬ï¼‰ï¼Œå†ç»†åˆ†æˆ TRAIN_BATCH_SIZE=20 çš„å°æ‰¹ï¼Œé…åˆæ¢¯åº¦ç´¯è®¡ ACCUMULATION_STEPS=13ã€‚

è¿™å¥—è®¾è®¡å…è®¸ï¼šæ˜¾å­˜é‡Œåªæ”¾å° batchï¼Œä½†é€»è¾‘ä¸Šåšå¤§ batch ä¼˜åŒ–ï¼Œå…¼é¡¾ç¨³å®šæ€§ä¸èµ„æºå ç”¨ã€‚

å†åŠ ä¸Šï¼š

TMPDIR="/dev/shm"ï¼šä¸­é—´æ–‡ä»¶æ”¾å†…å­˜ç›˜ï¼Œå‡è½» I/Oã€‚

USE_BF16 + DeepSpeed ZeRO-2ï¼šå‡æ˜¾å­˜ + å‡é€šä¿¡å¸¦å®½ã€‚

â†’ è¿™ä¸ºè®ºæ–‡ä¸­â€œèµ„æºæè‡´åˆ©ç”¨ / è½»é‡åŒ–â€æä¾›äº†éå¸¸å…·ä½“çš„å®ç°ç»†èŠ‚ã€‚

2.2 Experience & ReplayBufferActorï¼ˆç»éªŒæŠ½è±¡ä¸å¼‚æ­¥é‡‡æ ·ï¼‰

è¿ç»­ç‰ˆçš„ Experience ç»“æ„ï¼š

@dataclass
class Experience:
    obs: Dict[str, torch.Tensor]      # CPU ç«¯çš„è§‚æµ‹å­—å…¸
    action: np.ndarray                # æ ‡å‡†åŒ–åŠ¨ä½œï¼ˆtanh ä¹‹åï¼Œ[-1, 1]ï¼‰
    advantage: float                  # GAE è®¡ç®—å‡ºçš„ä¼˜åŠ¿
    behaviour_mu: np.ndarray          # è¡Œä¸ºç­–ç•¥çš„å‡å€¼
    behaviour_log_std: np.ndarray     # è¡Œä¸ºç­–ç•¥çš„ log_std
    behaviour_value: float            # è¡Œä¸ºç­–ç•¥çš„ V(s)


è§‚æµ‹ obs æ˜¯å·²ç»é€šè¿‡ prepare_one_obs é¢„å¤„ç†åçš„ CPU Tensor å­—å…¸ï¼Œæ–¹ä¾¿åé¢æ‰¹é‡ pad/stackã€‚

åŠ¨ä½œä¿¡æ¯ä»¥ (action, mu, log_std) å­˜åœ¨ ReplayBuffer ä¸­ï¼Œè®­ç»ƒæ—¶å¯ä»¥å®Œå…¨é‡å»ºè¡Œä¸ºç­–ç•¥ log probï¼Œåš PPO ratioã€‚

ReplayBufferActor æ˜¯ä¸€ä¸ªéå¸¸è½»é‡çš„ Ray actorï¼š

å†…éƒ¨å°±æ˜¯ deque(maxlen=REPLAY_CAPACITY)ã€‚

add_batch(batch)ï¼šappend ä¸€æ‰¹ Experienceã€‚

sample(batch_size)ï¼š

random.sample ä» buffer é€‰ batchã€‚

è¿”å› obs_list (list of dict)ã€actã€advã€mu_oldã€log_std_oldã€v_oldã€‚

éé˜»å¡ & çº¿ç¨‹å®‰å…¨ï¼šä½œä¸ºå•çº¿ç¨‹ Ray actorï¼Œæœ¬èº«å¤©ç„¶ä¸²è¡Œæ‰§è¡Œï¼Œä½†å„ worker å¼‚æ­¥å¾€å®ƒå‘è¿œç¨‹è°ƒç”¨ï¼›å¯¹ä¸Šå±‚æ¥è®²æ˜¯â€œå¼‚æ­¥ç®¡é“çš„ä¸€ç¯â€ã€‚

2.3 RolloutWorkerActorï¼šç¯å¢ƒäº¤äº’ + GAE è®¡ç®—

RolloutWorkerActor çš„å…³é”®ç‚¹ï¼š

ç¯å¢ƒä¸å¤„ç†å™¨åˆå§‹åŒ–

ä½¿ç”¨ LiberoEnvWrapper åŒ…è£… LIBERO ä»»åŠ¡ã€‚

æ¯ä¸ª worker æŒæœ‰ä¸€ä¸ª processorï¼ˆæ¥è‡ª OpenVLAï¼‰ï¼Œä½†ä¸åŠ è½½å®Œæ•´æ¨¡å‹ï¼Œå‡è½»æ˜¾å­˜å‹åŠ›ã€‚

ä¸»å¾ªç¯ run()

æ¯ä¸€è½®ï¼š

ç”¨ prepare_one_obs + step_count æ„é€ å•æ­¥è¾“å…¥ã€‚

è°ƒç”¨ self.infer.request.remote(inputs_t) â†’ åŒæ­¥ ray.get è·å¾—ï¼š

action_envï¼ˆå·² unnormalizeï¼Œå¯ç›´æ¥å–‚ç¯å¢ƒï¼‰

action_normï¼ˆå½’ä¸€åŒ–åŠ¨ä½œï¼Œç”¨äºè®­ç»ƒï¼‰

mu, log_std, value

å¯¹ action_env é€æ­¥ä¸ç¯å¢ƒäº¤äº’ï¼ŒæŒ‰ chunk æ”¶é›† (obs, action_norm, reward_scaled, mu, log_std, value) è¿›å…¥ self.local_bufferã€‚

ä¸€æ—¦ï¼š

episode ç»“æŸ â†’ _process_traj(self.local_buffer, bootstrap_val=0.0)ï¼Œå¹¶æ¸… bufferã€‚

buffer é•¿åº¦è¾¾åˆ° ROLLOUT_LOCAL_BUF + 1 â†’ ç”¨æœ€åä¸€ä¸ª value åš bootstrapï¼ŒGAE ä¸€æ¬¡æ€§å›çŒã€‚

GAE è®¡ç®— _process_traj

é€†åºéå†è½¨è¿¹ï¼Œç”¨

ğ›¿
ğ‘¡
=
ğ‘Ÿ
ğ‘¡
+
ğ›¾
ğ‘‰
ğ‘¡
+
1
âˆ’
ğ‘‰
ğ‘¡
,
GAE
ğ‘¡
=
ğ›¿
ğ‘¡
+
ğ›¾
ğœ†
GAE
ğ‘¡
+
1
Î´
t
	â€‹

=r
t
	â€‹

+Î³V
t+1
	â€‹

âˆ’V
t
	â€‹

,GAE
t
	â€‹

=Î´
t
	â€‹

+Î³Î»GAE
t+1
	â€‹


å°†å¾—åˆ°çš„ä¼˜åŠ¿ adv å­˜å…¥ Experienceï¼›value æœ¬èº«ä½œä¸º behaviour_valueï¼Œç”¨äºæ„é€  value lossã€‚

å°†æ•´ä¸ª batch é€šè¿‡ self.replay.add_batch.remote(batch) å¼‚æ­¥å†™å…¥ ReplayBufferActorã€‚

è¿™é‡Œä¸€ä¸ªé‡è¦å–ç‚¹æ˜¯ï¼šä¼˜åŠ¿ä¼°è®¡å®Œå…¨åœ¨ rollout ä¾§å®Œæˆï¼ŒTrainer ç›´æ¥æ¶ˆè´¹ä¼˜åŠ¿ï¼Œä»è€Œå‡è½»è®­ç»ƒç«¯çš„ compute loadï¼ŒæŠŠæ›´å¤šè®¡ç®—å‰ç§»åˆ° CPU ç«¯ / rollout ç«¯ã€‚

2.4 InferenceActorï¼šå¼‚æ­¥æ‰¹å¤„ç†æ¨ç†ï¼ˆå¼‚æ­¥æ€§çš„æ ¸å¿ƒï¼‰

InferenceActor ç»§æ‰¿è‡ª InferenceActorComï¼Œå…³é”®ç»“æ„ï¼š

åˆå§‹åŒ–æ—¶ï¼š

åœ¨ GPU ä¸ŠåŠ è½½ ActorCriticï¼ˆè¿ç»­ç‰ˆæœ¬ï¼‰ã€‚

è®°å½• batch_size = INFERENCE_BATCH å’Œ timeout_sec = INFERENCE_TIMEOUT_MS / 1000ã€‚

å»ºç«‹ä¸¤ä¸ªé˜Ÿåˆ—ï¼š

self.requests: List[inputs_dict]

self.promises: List[asyncio.Future]

åœ¨å½“å‰äº‹ä»¶å¾ªç¯é‡Œåˆ›å»ºåå°ä»»åŠ¡ï¼š

self._bg_task = loop.create_task(self._loop())
self._bg_task.add_done_callback(self._on_bg_task_done)


å‰ç«¯æ¥å£ request(self, inputs)

åˆ›å»ºä¸€ä¸ª asyncio.Future æ”¾è¿› self.promisesï¼Œinputs æ”¾è¿› self.requestsã€‚

ç›´æ¥è¿”å›è¿™ä¸ª futureï¼Œç”±è°ƒç”¨æ–¹å†³å®š ray.get ç­‰å¾…ç»“æœã€‚

RolloutWorker è°ƒç”¨ infer.request.remote(...)ï¼Œç­‰ä»·äºâ€œRPC + Futureâ€ã€‚

åå°å¾ªç¯ _loopï¼ˆå¼‚æ­¥æ‰¹å¤„ç†é€»è¾‘ï¼‰

ä¸æ–­æ£€æŸ¥ï¼š

å¦‚æœ len(requests) >= batch_size æˆ–è€… å½“å‰æ—¶é—´ - last_process_time > timeout_secï¼š

å°†å½“å‰ requests å’Œ promises å¼¹å‡ºï¼Œå½¢æˆä¸€ä¸ªæ‰¹æ¬¡ã€‚

æŠŠæ‰€æœ‰ inputs ç”¨ prepare_inputs_batch å †å æˆå¤§ batchã€‚

åœ¨ GPU ä¸Šå‰å‘ä¸€æ¬¡ï¼Œå¾—åˆ°ï¼š

æ ‡å‡†åŒ–åŠ¨ä½œ

mu, log_std, values

å°†æ ‡å‡†åŒ–åŠ¨ä½œ clip åˆ° [-1,1]ï¼Œå†ç”¨ vla._unnormalize_actions æ˜ å°„å›ç¯å¢ƒåŠ¨ä½œã€‚

å°†æ¯æ¡æ ·æœ¬çš„ (action_env[i], actions_norm[i], mu[i], log_std[i], values[i])
é€šè¿‡å¯¹åº”çš„ promise.set_result(...) è¿”å›ã€‚

å¦‚æœå‰å‘è¿‡ç¨‹æŠ¥é”™ï¼Œä¼šæ•è·å¼‚å¸¸ï¼Œæ‰“å°å †æ ˆï¼Œå¹¶å¯¹æ‰€æœ‰æœªå®Œæˆçš„ promises è°ƒç”¨ set_exceptionï¼Œé¿å…ä¸Šæ¸¸æ­»ç­‰ã€‚

è¿™éƒ¨åˆ†æ˜¯æ•´ä¸ªæ¡†æ¶ â€œå¼‚æ­¥æ¨ç† + èµ„æºæè‡´åˆ©ç”¨â€ çš„æ ¸å¿ƒï¼š

å¤šä¸ª rollout worker çš„è¯·æ±‚è¢«ç»Ÿä¸€æ‰“åŒ…ï¼Œæœ€å¤§åŒ– GPU åˆ©ç”¨ç‡ã€‚

é€šè¿‡ timeout æœºåˆ¶åœ¨ååå’Œå»¶è¿Ÿä¹‹é—´æŠ˜ä¸­ã€‚

å‰åç«¯ç”¨ Future è§£è€¦ï¼ŒRolloutWorker è§†è§’ä¸Šåªçœ‹åˆ°ä¸€ä¸ªè¿œç«¯â€œæ¨ç†æœåŠ¡â€ã€‚

2.5 TrainerActorï¼šDeepSpeed + å¼‚æ­¥æ•°æ®åŠ è½½ + PPO è®­ç»ƒ

TrainerActor çš„äº®ç‚¹ä¸»è¦åœ¨ï¼š

(1) DeepSpeed åˆå§‹åŒ– & è½»é‡åŒ–å¾®è°ƒ

é€šè¿‡ deepspeed.initialize(model=model, config=ds_config, model_parameters=optimizer_params)ï¼š

model æ˜¯ ActorCriticï¼Œå†…éƒ¨ä½¿ç”¨ LoRA å¾®è°ƒ OpenVLA ä¸»å¹²ï¼Œä»…å°‘é‡å‚æ•°å¯è®­ç»ƒï¼ˆæ˜¾è‘—å‡å°‘æ˜¾å­˜å ç”¨ï¼‰ã€‚

ä¼˜åŒ–å™¨å‚æ•°åˆ†ä¸º policy å’Œ value ä¸¤ä¸ª param groupï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦ã€‚

DeepSpeed ZeRO-2 é…ç½®å¯ç”¨äº†ï¼š

overlap_comm=Trueï¼Œé€šä¿¡ä¸è®¡ç®—é‡å ï¼›

reduce_scatter=True ç­‰ï¼Œé™ä½é€šä¿¡å¼€é”€ã€‚

è„šæœ¬ä¸­æ‰“å°ï¼š

æ€»å‚æ•°é‡: n_total, å¯è®­ç»ƒå‚æ•°é‡: n_trainable


å¼ºè°ƒâ€œä¸»ä½“å‚æ•°å†»ç»“ + å°é‡å¯è®­ç»ƒå‚æ•°â€çš„è½»é‡ç‰¹æ€§ã€‚

(2) å¼‚æ­¥æ•°æ®å‡†å¤‡ _data_fetching_loop

å¯åŠ¨æ—¶ self.data_fetching_task = asyncio.get_event_loop().create_task(self._data_fetching_loop())ã€‚

_data_fetching_loop é€»è¾‘ï¼š

å¦‚æœ self.next_ready_batch å·²ç»æœ‰æ•°æ®ï¼Œåˆ™ await asyncio.sleep(0.1)ï¼Œé¿å…è¶…å‰å–æ•°ã€‚

è°ƒç”¨ await self.replay_buffer.size.remote() æ£€æŸ¥ ReplayBuffer å¤§å°ï¼Œä¸è¶³ SUPER_BATCH_SIZE å°±ç¡ 3 ç§’é‡è¯•ã€‚

ä¸€æ—¦æ•°æ®è¶³å¤Ÿï¼Œè°ƒç”¨ await self.replay_buffer.sample.remote(self.super_batch_size) æ‹‰å–è¶…çº§æ‰¹æ¬¡ã€‚

ç”¨ base_model.prepare_inputs_batch(obs_list) åšæ‰¹é‡ pad/stackã€‚

å°† numpy æ•°æ®è½¬æ¢ä¸º torch.Tensorï¼Œè¿ç§»åˆ° self.model çš„ deviceã€‚

å°† (inputs_batch, act_t, adv_t, mu_old_t, log_std_old_t, v_old_t) å¡è¿› self.next_ready_batchã€‚

è¿™å®ç°äº†ä¸€ä¸ªç»å…¸çš„ åŒç¼“å†² / é¢„å–ç®¡çº¿ï¼š

å‰å° run_training_epoch æ­£åœ¨åƒ current_batch æ—¶ï¼Œ

åå° _data_fetching_loop å·²ç»åœ¨å¹¶è¡Œå‡†å¤‡ä¸‹ä¸€ä¸ª super batchã€‚

(3) PPO è®­ç»ƒé€»è¾‘ run_training_epoch

æ ¸å¿ƒæ­¥éª¤ï¼š

ç­‰å¾… next_ready_batch å‡†å¤‡å¥½ï¼ˆé¦–ä¸ªå‘¨æœŸä¼š block ä¸€æ¬¡ï¼‰ã€‚

æ‹‰å‡º current_batchï¼ŒåŒæ—¶æ¸…ç©º next_ready_batchï¼Œè®©åå°ç»§ç»­ fetchã€‚

æ ¹æ® adv_t + v_old_t æ„é€  v_targ_tï¼Œå¹¶åœ¨å…¨å±€èŒƒå›´åš advantage çš„ all-reduce æ ‡å‡†åŒ–ï¼š

å…ˆåœ¨æœ¬æœºæ±‚ local_sum, local_sq_sum, local_countã€‚

distributed.all_reduce èšåˆåˆ°æ‰€æœ‰ trainer rankã€‚

ç®—å‡º global_mean, global_stdï¼Œå¯¹ä¼˜åŠ¿åšæ ‡å‡†åŒ–ã€‚

å°†è¶…çº§æ‰¹æ¬¡æŒ‰ TRAIN_BATCH_SIZE æ‹†æˆå¤šä¸ªå° batchï¼Œå¾ªç¯ï¼š

å‰å‘å¾—åˆ°æ–°ç­–ç•¥åˆ†å¸ƒå’Œ valueã€‚

è®¡ç®— PPO æŸå¤±ï¼špolicy loss + value loss + entropy regularization + KL penalty ç­‰ã€‚

è°ƒç”¨ self.model.backward(loss) + æ¢¯åº¦è£å‰ª + self.model.step()ã€‚

ä½¿ç”¨è‡ªå®šä¹‰ _get_current_lr åš warmup + cosine decayï¼Œåˆ†å¼€å¯¹ policyã€value ç»„è°ƒæ•´å­¦ä¹ ç‡ã€‚

è®­ç»ƒè¿‡ç¨‹æœ¬èº«æ˜¯å…¸å‹çš„ synchronous data-parallelï¼ˆç”¨ all-reduce åšæ¢¯åº¦èšåˆï¼‰ï¼Œ
ä½†ä¸ ReplayBuffer / Rollout / æ¨ç†ä¹‹é—´æ˜¯ å¼‚æ­¥è§£è€¦ çš„ã€‚

2.6 main()ï¼šè®­ç»ƒå¾ªç¯ä¸æƒé‡å¹¿æ’­

ä¸»å‡½æ•°å¤§è‡´æµç¨‹ï¼š

Ray åˆå§‹åŒ–ï¼Œå¯åŠ¨å¤šä¸ª TrainerActorã€InferenceActorã€ReplayBufferActorã€RolloutWorkerActorã€StatsActor ç­‰ã€‚

è®­ç»ƒ group å†…éƒ¨ä½¿ç”¨ DeepSpeed / torch.distributed è‡ªå·±çš„è¿›ç¨‹ç»„ï¼›
æ¨ç†å¹¿æ’­ç»„ä½¿ç”¨ ds_com åˆ›å»ºçš„é¢å¤– groupï¼ˆç‹¬ç«‹ master_portï¼‰ã€‚

é€šè¿‡ TrainerActorCom.get_broadcast_signature() ä¸ InferenceActorCom.get_broadcast_signature() æ¯”è¾ƒå‚æ•°/ç¼“å†²åŒºçš„ç­¾åï¼Œç¡®ä¿ç»“æ„ä¸€è‡´ã€‚

åˆæ¬¡å¹¿æ’­ï¼š

trainer_group[0].broadcast_weights.remote(BROADCAST_GROUP_NAME)ã€‚

InferenceActor.receive_and_update_weights.remote(BROADCAST_GROUP_NAME)ã€‚

å¯åŠ¨ RolloutWorkers å’Œ EvaluationWorkers çš„ run.remote()ï¼Œå®ƒä»¬åœ¨åå°æ— é™å¾ªç¯ã€‚

ç­‰å¾… ReplayBuffer å…ˆå¡«æ»¡ä¸€å®šé‡æ•°æ®ï¼ˆwarmup é˜¶æ®µï¼‰ã€‚

ä¸»è®­ç»ƒ loopï¼š

å¹¶è¡Œå‘èµ·æ‰€æœ‰ trainer çš„ run_training_epoch.remote()ã€‚

ä½¿ç”¨ ray.get ç­‰å¾…ä¸€æ¬¡ epoch å®Œæˆï¼Œæ‹¿åˆ° loss / ç»Ÿè®¡ä¿¡æ¯ / global_stepã€‚

ç”¨ broadcast_weights â†’ receive_and_update_weights å°†æœ€æ–°ç­–ç•¥åŒæ­¥åˆ°æ‰€æœ‰æ¨ç†å™¨ã€‚

å®šæœŸä» StatsActor æ‹‰å–ç»Ÿè®¡ï¼Œè®¡ç®—è®­ç»ƒé€Ÿåº¦ steps/sec å’Œå„ç¯å¢ƒçš„å¹³å‡å›æŠ¥/æˆåŠŸç‡ï¼Œå†™ TensorBoardã€‚

åœ¨è¿™ä¸ªç²’åº¦ä¸Šï¼Œæ¯ä¸ª â€œepochâ€ ä»ç„¶æ˜¯åŒæ­¥çš„ï¼ˆæ‰€æœ‰ TrainerActor ä¸€èµ·è¿›é€€ï¼‰ï¼Œ
ä½† å’Œ rollout ä¾§æ˜¯å®Œå…¨å¹¶è¡Œçš„ï¼šrollout æ°¸è¿œåœ¨å‰å°è·‘ï¼Œtrainer åªæ˜¯åœ¨åå°å‘¨æœŸæ€§åœ°æ¶ˆè´¹ buffer ä¸­çš„æ•°æ®ã€‚

ä¸‰ã€ç¦»æ•£åŠ¨ä½œç‰ˆï¼šds_libero_ppo_discrete.py å…³é”®å·®å¼‚

ç¦»æ•£ç‰ˆåœ¨æ•´ä½“æ¶æ„ä¸Šä¸è¿ç»­ç‰ˆå‡ ä¹ä¸€è‡´ï¼Œä½†æœ‰ä¸‰ä¸ªé‡è¦å·®å¼‚ç‚¹ï¼Œä¸è®ºæ–‡ Methodology ä¸­ â€œé€šç”¨å¼‚æ­¥æ¡†æ¶ + å¤šåŠ¨ä½œç©ºé—´æ”¯æŒâ€ å¼ºç›¸å…³ï¼š

3.1 Experience ç»“æ„ä¸ä¼˜åŠ¿/å›æŠ¥

ç¦»æ•£ Experienceï¼š

@dataclass
class Experience:
    obs: Dict[str, torch.Tensor]
    action_token: np.ndarray           # ç¦»æ•£åŠ¨ä½œ token å‘é‡
    advantage: float
    behaviour_logits: np.ndarray       # è¡Œä¸ºç­–ç•¥ logits
    value_target: float               # V targetï¼Œè€Œä¸æ˜¯ behaviour_value


åœ¨ _process_traj ä¸­åŒæ—¶è®¡ç®—ï¼š

é€†åº GAE â†’ advï¼›

ret = adv + V â†’ value_targetã€‚

Trainer åœ¨è¯»å–æ—¶ç›´æ¥æ‹¿ adv å’Œ value_targetï¼Œä¸å†ä¾èµ– behaviour_valueã€‚

3.2 RolloutWorkerActor çš„ ä»»åŠ¡é‡‡æ ·ç­–ç•¥ï¼ˆè½»é‡ç‰ˆ curriculum learningï¼‰

åœ¨ç¦»æ•£ç‰ˆçš„ RolloutWorkerActor ä¸­ï¼Œå¤šä»»åŠ¡ LIBERO ç¯å¢ƒé€šè¿‡ä»¥ä¸‹æ–¹å¼é€‰æ‹©ï¼š

failure_counts = np.array([sum(history) for history in self.env_outcome])
env_weights = failure_counts + 1
probabilities = env_weights / np.sum(env_weights)
self.current_env_idx = np.random.choice(self.num_tasks, p=probabilities)


env_outcome[i] è®°å½•è¯¥ task æœ€è¿‘è‹¥å¹² episode çš„å¤±è´¥æƒ…å†µã€‚

å¤±è´¥è¶Šå¤š â†’ æƒé‡è¶Šé«˜ â†’ æ›´å®¹æ˜“è¢«é‡‡æ ·ã€‚

è¿™å½¢æˆäº†ä¸€ä¸ª è½»é‡çº§ä»»åŠ¡è‡ªé€‚åº”è°ƒåº¦ç­–ç•¥ï¼Œå®Œå…¨åœ¨ worker ç«¯æœ¬åœ°å®ç°ï¼Œä¸éœ€è¦é¢å¤–çš„è°ƒåº¦æœåŠ¡ã€‚

å–ç‚¹ï¼šåˆ©ç”¨æä½ä»£ä»·åœ¨ rollout ç«¯å®ç°äº†åŠ¨æ€ä»»åŠ¡é‡‡æ ·ï¼Œæé«˜ sample efficiencyï¼Œä¸”ä¸å¼‚æ­¥æ¶æ„å…¼å®¹ã€‚

3.3 æ¨ç†ä¸è®­ç»ƒç»†èŠ‚å·®å¼‚

æ¨ç†ç«¯ InferenceActor ä½¿ç”¨çš„æ˜¯ ActorCritic çš„ ç¦»æ•£åŠ¨ä½œç‰ˆæœ¬ï¼š

è¾“å‡ºæ˜¯ä¸€ç»„ç¦»æ•£ logitsï¼Œä½¿ç”¨ Categorical é‡‡æ ·ã€‚

Trainer ä» ReplayBuffer æ‹¿åˆ°ï¼š

action_token_npï¼šæ¯ä¸ªç»´åº¦æ˜¯ä¸€ä¸ªç¦»æ•£ tokenã€‚

behaviour_logitsï¼šåŸå§‹è¡Œä¸ºç­–ç•¥ logitsã€‚

PPO æŸå¤±ï¼š

è¡ŒåŠ¨æ¦‚ç‡æ¥è‡ª softmax(logits)ï¼Œä½¿ç”¨äº¤å‰ç†µæˆ– log-prob å¯¹æ¯” action_tokenã€‚

ratio åŸºäºè¡Œä¸ºç­–ç•¥ logits è®¡ç®—çš„ old log-probï¼Œä¸å½“å‰ç­–ç•¥çš„ log-prob åšå‰ªåˆ‡ã€‚

æ­¤å¤–ï¼Œç¦»æ•£ç‰ˆå¢åŠ äº† EvaluationWorkerActorï¼š

ä¸ RolloutWorkerActor ç±»ä¼¼ï¼Œä½†ï¼š

ä¸å‘ ReplayBuffer å†™æ•°æ®ã€‚

ä¸“é—¨ç”¨äºå‘¨æœŸæ€§è¯„ä¼°å½“å‰ç­–ç•¥çš„æˆåŠŸç‡ã€‚

è¯„ä¼° worker ä¸ rollout worker åŒæ ·å¼‚æ­¥è¿è¡Œï¼Œå¯¹è®­ç»ƒæ— é˜»å¡ã€‚

å››ã€é€šä¿¡æœºåˆ¶ä¸å¼‚æ­¥ç‰¹æ€§æ€»ç»“ï¼ˆå…¨å±€è§†è§’ï¼‰
4.1 è®­ç»ƒå™¨ â†” æ¨ç†å™¨æƒé‡å¹¿æ’­ï¼ˆds_comï¼‰

TrainerActorCom / InferenceActorCom å°è£…äº†ä»¥ä¸‹è®¾è®¡ï¼š

ä½¿ç”¨å•ç‹¬çš„ collective process groupï¼ˆå¯é€‰ GLOO / NCCL åç«¯ï¼‰åšæƒé‡åŒæ­¥ï¼Œä¸è®­ç»ƒ all-reduce ç»„è§£è€¦ã€‚

Trainer ç«¯ï¼š

ç”¨ GatheredParameters åœ¨ ZeRO-2 ä¸‹å…ˆèšåˆå®Œæ•´å‚æ•°åˆ° rank 0ã€‚

éå† module.named_parameters(recurse=True) å’Œ named_buffers æ‰“åŒ…ä¸ºè¿è´¯ tensor åˆ—è¡¨ï¼Œè°ƒç”¨ dist.broadcastã€‚

Inference ç«¯ï¼š

ç”¨åŒæ ·çš„é¡ºåºå’Œ dtype åˆ†é…ä¸´æ—¶ bufferã€‚

æ¥æ”¶å¹¿æ’­ç»“æœï¼Œå†æŒ‰é¡ºåºå†™å› model ä¸­çš„å‚æ•° / bufferã€‚

get_broadcast_signature æä¾›ï¼ˆåç§°, å½¢çŠ¶, dtypeï¼‰åˆ—è¡¨ï¼Œç”¨äºåˆå§‹åŒ–é˜¶æ®µçš„å¯¹é½æ£€æŸ¥ã€‚

å¯¹è®ºæ–‡è€Œè¨€ï¼Œå¯ä»¥æè¿°ä¸ºï¼š

è®­ç»ƒé€šä¿¡ï¼ˆæ¢¯åº¦ all-reduceï¼‰ä¸æ¨ç†å¹¿æ’­é€šä¿¡ä½¿ç”¨ä¸åŒè¿›ç¨‹ç»„éš”ç¦»ï¼›

å¯¹æ¨ç†ä¾§é‡‡ç”¨ push-based å‚æ•°åŒæ­¥ï¼ˆè€Œéå‚æ•°æœåŠ¡å™¨æ‹‰å–ï¼‰ï¼Œç®€å•è½»é‡ã€‚

4.2 æ•°æ®æµï¼šä»ç¯å¢ƒåˆ°æ¢¯åº¦æ›´æ–°çš„å¼‚æ­¥ç®¡çº¿

å¯ä»¥ç”¨ä¸€ä¸ªç®€åŒ–çš„æµæ°´çº¿è¡¨ç¤ºï¼š

RolloutWorkerActorï¼ˆCPU & ç¯å¢ƒï¼‰

ä¸ç¯å¢ƒäº¤äº’ï¼Œç§¯ç´¯ (obs, action, reward, value)ã€‚

æœ¬åœ° GAE â†’ ä¼˜åŠ¿ / å›æŠ¥ã€‚

replay.add_batch.remote(batch)ï¼ˆå¼‚æ­¥ï¼‰ã€‚

ReplayBufferActorï¼ˆå•çº¿ç¨‹ Queueï¼‰

ç¼“å­˜ Experienceï¼Œæ”¯æŒéšæœºé‡‡æ ·ã€‚

TrainerActorï¼ˆGPUï¼Œå¤šè¿›ç¨‹ï¼‰

åå°åç¨‹ _data_fetching_loop ä¸æ–­ sample(super_batch_size)ã€‚

å‰å°åç¨‹ run_training_epoch ä½¿ç”¨ä¸Šä¸€ä¸ª ready batch åšå¤šæ­¥ PPO æ›´æ–°ã€‚

InferenceActorï¼ˆGPUï¼‰

èƒŒæ™¯åç¨‹ _loop èšåˆæ¨ç†è¯·æ±‚ï¼Œç»Ÿä¸€ forwardã€‚

RolloutWorker è§†ä¸ºè°ƒç”¨ä¸€ä¸ªâ€œæ¨ç†æœåŠ¡â€ã€‚

åœ¨è¿™ä¸ª pipeline ä¸­ï¼š

ç¯å¢ƒäº¤äº’ã€æ¨ç†å‰å‘ã€è®­ç»ƒæ›´æ–° ä¸‰æ¡é“¾è·¯é«˜åº¦é‡å ï¼Œé  Ray çš„ actor æŠ½è±¡ + asyncio åç¨‹ä¸²èµ·æ¥ã€‚

å•ä¸ªç»„ä»¶å´©æºƒæ—¶ï¼Œä¼šæ‰“å°è¯¦ç»†å †æ ˆå¹¶è®© Ray æ ‡è®°å¤±è´¥ï¼Œä¾¿äº debugã€‚

äº”ã€ä»£ç å±‚é¢çš„åˆ›æ–°ç‚¹ & è®ºæ–‡å¯ç”¨å–ç‚¹æ•´ç†

ç»“åˆä½ å¸Œæœ›çªå‡ºçš„ä¸‰ç‚¹ï¼Œæˆ‘å¸®ä½ å…ˆä»ä»£ç ç»´åº¦æ•´ç†ä¸€ä¸ªâ€œå¯ç›´æ¥å†™è¿›è®ºæ–‡â€çš„å–ç‚¹åˆ—è¡¨ï¼Œåç»­åœ¨ Methodology å¯ä»¥å±•å¼€ï¼š

5.1 èµ„æºæè‡´åˆ©ç”¨ï¼ˆResource Efficiencyï¼‰

æ¨ç†ç«¯æ‰¹å¤„ç† + å¼‚æ­¥é˜Ÿåˆ—

é€šè¿‡ INFERENCE_BATCH + INFERENCE_TIMEOUT_MS åšè‡ªé€‚åº” batchï¼Œå°½é‡ç”¨æ»¡ GPUã€‚

è®­ç»ƒç«¯åŒç¼“å†²è¶…çº§æ‰¹æ¬¡

_data_fetching_loop ä¸è®­ç»ƒå¾ªç¯è§£è€¦ï¼Œåœ¨ I/O / æ•°æ®å¤„ç†ä¸åå‘ä¼ æ’­ä¹‹é—´å»ºç«‹ pipelineã€‚

å¤šçº§å¹¶è¡Œ

ç¯å¢ƒå¹¶è¡Œï¼ˆ40 ä¸ª rollout workerï¼‰ + æ¨ç†æ‰¹å¤„ç† + å¤š GPU æ•°æ®å¹¶è¡Œï¼ˆDeepSpeed ZeRO-2ï¼‰ã€‚

bf16 + LoRA + ZeRO-2

å¤§éƒ¨åˆ†å‚æ•°å†»ç»“ï¼Œä»… LoRA + value head è®­ç»ƒï¼Œæ˜¾å­˜/å¸¦å®½å¼€é”€éƒ½å¤§å¹…å‡å°ã€‚

5.2 è½»é‡åŒ–è®¾è®¡ï¼ˆLightweight Systemï¼‰

ä½¿ç”¨ Ray Actors + DeepSpeed + PyTorch ä¸‰ä»¶å¥—ï¼Œæ²¡æœ‰é¢å¤–å¤æ‚çš„ RPC æ¡†æ¶æˆ–å‚æ•°æœåŠ¡å™¨ã€‚

ReplayBuffer æ˜¯ä¸€ä¸ªæç®€ç»“æ„ï¼ˆdeque + random.sampleï¼‰ï¼Œä½†é€šè¿‡ Ray å˜æˆåˆ†å¸ƒå¼å¯è§çš„æœåŠ¡ã€‚

é€šä¿¡æ¨¡å— ds_com ç”¨å°‘é‡ä»£ç å°è£…äº†å¹¿æ’­é€»è¾‘ï¼Œå¯é‡ç”¨äºå…¶ä»–è„šæœ¬ï¼ˆä¸–ç•Œæ¨¡å‹ã€GRPO ç­‰ï¼‰ã€‚

Rollout ç«¯ä¸åŠ è½½å®Œæ•´æ¨¡å‹ï¼Œåªéœ€è¦ processorï¼Œè¿›ä¸€æ­¥é™ä½å• worker å ç”¨ã€‚

5.3 å¼‚æ­¥å¼åˆ†å¸ƒè®­ç»ƒï¼ˆAsynchronous Distributed RLï¼‰

Rollout â†” Trainer å¼‚æ­¥ï¼š

RolloutWorkers æ°¸è¿œåœ¨åå°è·‘ï¼Œä¸å› è®­ç»ƒæ…¢è€Œåœï¼›Trainer åªæ˜¯å¶å°”ä» ReplayBuffer æ‹‰å–ä¸€å¤§æ‰¹æ•°æ®ã€‚

æ¨ç†æœåŠ¡å¼‚æ­¥ï¼š

InferenceActor çš„ request/future æœºåˆ¶ + _loop æ‰¹å¤„ç†ï¼Œæ˜¯æ ‡å‡†çš„å¼‚æ­¥ RPC â†’ batched inference æ¨¡å¼ã€‚

è®­ç»ƒå†…éƒ¨å¼‚æ­¥æ•°æ®é¢„å–ï¼š

_data_fetching_loop ä¸ run_training_epoch çš„åˆ†ç¦»æ˜¯å…¸å‹çš„ producerâ€“consumer æ¨¡å¼ã€‚

åŒæ—¶ä¿æŒï¼š

è®­ç»ƒå™¨å†…éƒ¨çš„æ¢¯åº¦åŒæ­¥ä»ç„¶æ˜¯åŒæ­¥çš„ï¼ˆé€šè¿‡ DeepSpeed all-reduceï¼‰ï¼Œ

ä»è€Œä¿è¯ä¼˜åŒ–è¿‡ç¨‹çš„ç†è®ºç¨³å®šæ€§ä¸å¯åˆ†ææ€§ã€‚