import os
import torch
import torch.nn as nn
from typing import Dict, Any, Tuple, List
import numpy as np
from peft import LoraConfig, get_peft_model

# Core OpenVLA components
from experiments.robot.openvla_utils import (
    get_action_head,
    get_processor,
    get_proprio_projector,
)

# Masks used to extract action-related hidden states
from prismatic.training.train_utils import (
    get_current_action_mask,
    get_next_actions_mask,
)

# Constants
from prismatic.vla.constants import (
    NUM_ACTIONS_CHUNK,
    ACTION_DIM,
    PROPRIO_DIM,
)
from typing import Any
import torch

from rl.utils import get_vla, compute_num_patches, prepare_inputs_batch, forward_vla


class ActorCritic(nn.Module):
    """
    Actor-Critic for OpenVLA-based continuous control.

    forward(inputs_batch) returns:
      - actions_all: sampled actions in (-1, 1), shape (B, NUM_ACTIONS_CHUNK, ACTION_DIM)  [squashed Gaussian]
      - mu_all: mean actions from action_head.predict_action(...), shape (B, NUM_ACTIONS_CHUNK, ACTION_DIM)
      - log_std_all: condition-independent log-std broadcast to all chunks, shape (B, NUM_ACTIONS_CHUNK, ACTION_DIM)
      - value: state value estimate, shape (B,)
    """

    def __init__(self, cfg, torch_dtype: torch.dtype):
        super().__init__()
        self.cfg = cfg

        # Device / dtype
        self.vla = get_vla(cfg)
        self.device = self.vla.device
        self.model_dtype = torch_dtype
        self.vla = self.vla.to(dtype=self.model_dtype)

        # ğŸ”’ å†»ç»“ VLA å‚æ•°
        for param in self.vla.parameters():
            param.requires_grad = False

        # æ ‡è®° VLA æ˜¯å¦å·²ç»è¢« LoRA ä¿®æ”¹
        self._vla_is_lora_tuned = False

        # ä¿ç•™ processorï¼ˆåªæ˜¯é¢„å¤„ç†ï¼Œä¸éœ€è¦è®­ç»ƒï¼‰
        self.processor = get_processor(cfg)

        # Heads
        self.action_head = get_action_head(cfg, llm_dim=self.vla.llm_dim)
        self.action_head = self.action_head

        self.proprio_projector = get_proprio_projector(
            cfg, llm_dim=self.vla.llm_dim, proprio_dim=PROPRIO_DIM
        )

        # Condition-independent log_std parameter (float32 for stability)
        # self.log_std_param = nn.Parameter(torch.full((NUM_ACTIONS_CHUNK, ACTION_DIM), -2, dtype=self.model_dtype, device=self.device))
        # self.log_std_param = L1RegressionActionHead(input_dim=self.vla.llm_dim, hidden_dim=self.vla.llm_dim, action_dim=ACTION_DIM)
        self.register_buffer('log_std_param', torch.full((NUM_ACTIONS_CHUNK, ACTION_DIM), -2.0, dtype=self.model_dtype))

        self.attn_pool = nn.Linear(self.vla.llm_dim, 1)

        self.step_count_emb =nn.Embedding(500,4096)

        # Value head: mean-pool over text tokens from the last hidden layer -> scalar
        self.value_head = nn.Sequential(
            nn.LayerNorm(self.vla.llm_dim),
            nn.Linear(self.vla.llm_dim, self.vla.llm_dim),
            nn.ReLU(),
            nn.Linear(self.vla.llm_dim, 1),
        )
        self.setup_finetuning(cfg.lora_rank, cfg.lora_dropout)

        self.to(self.device, dtype=self.model_dtype)

    def setup_finetuning(self, lora_rank: int, lora_dropout: float):
        """ä¸ºå¾®è°ƒå‡†å¤‡æ¨¡å‹ï¼Œæ³¨å…¥ LoRA é€‚é…å™¨ã€‚"""
        if self.cfg.use_lora:
            print("Injecting LoRA adapters for fine-tuning...")
            lora_config = LoraConfig(
                r=lora_rank,
                lora_alpha=min(lora_rank, 16),
                lora_dropout=lora_dropout,
                target_modules="all-linear",
                init_lora_weights="gaussian",
            )
            self.vla = get_peft_model(self.vla, lora_config)
            self._vla_is_lora_tuned = True
            print("LoRA injection complete.")
            self.vla.print_trainable_parameters()

    def get_parameter_groups(self) -> List[Dict[str, Any]]:
        """
        å°†å¯è®­ç»ƒå‚æ•°åˆ†ä¸º 'policy' å’Œ 'value' ä¸¤ç»„ã€‚
        è¿™å¯¹äºä¸ºä¸åŒç»„ä»¶è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡è‡³å…³é‡è¦ã€‚
        """
        policy_params = list(self.action_head.parameters()) + list(self.proprio_projector.parameters())
        value_params = list(self.value_head.parameters()) + list(self.attn_pool.parameters()) + list(self.step_count_emb.parameters())

        if self._vla_is_lora_tuned:
            lora_params = [p for p in self.vla.parameters() if p.requires_grad]
            policy_params += lora_params
        
        # ç¡®ä¿æ²¡æœ‰é—æ¼ä»»ä½•å¯è®­ç»ƒå‚æ•°
        all_trainable_params = set(filter(lambda p: p.requires_grad, self.parameters()))
        grouped_params = set(policy_params) | set(value_params) | (set(lora_params) if self._vla_is_lora_tuned else set())
        assert all_trainable_params == grouped_params, "å¹¶éæ‰€æœ‰å¯è®­ç»ƒå‚æ•°éƒ½è¢«åˆ†ç»„ï¼"

        return [
            {"name": "policy", "params": policy_params},
            {"name": "value", "params": value_params},
        ]

    def _extract_actions_hidden(self, last_hidden_states: torch.Tensor, labels, has_act_emb) -> torch.Tensor:
        """
        From last_hidden_states, extract the text-token hiddens corresponding
        to current + next actions, as (B, NUM_ACTIONS_CHUNK*ACTION_DIM, D).
        """
        ground_truth_token_ids = labels[:, 1:].to(self.device)  # (B, text_len-1)
        current_action_mask = get_current_action_mask(ground_truth_token_ids)  # (B, text_len-1)
        next_actions_mask = get_next_actions_mask(ground_truth_token_ids)      # (B, text_len-1)
        action_mask = current_action_mask | next_actions_mask

        num_patches = self._compute_num_patches()
        if has_act_emb:
            num_patches += 1
        text_hidden_states = last_hidden_states[:, num_patches:-1]  # (B, text_len, D)

        B, _, D = text_hidden_states.shape
        actions_hidden_states = (
            text_hidden_states[action_mask]
            .reshape(B, NUM_ACTIONS_CHUNK * ACTION_DIM, D)
            .to(self.model_dtype)
        )
        return actions_hidden_states

    def _forward_vla(self, batch: Dict[str, torch.Tensor]):
        return forward_vla(self, batch)

    def _compute_value_from_hidden(self, actions_hidden_states: torch.Tensor, step_counts_batch: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨æ³¨æ„åŠ›æ± åŒ–è®¡ç®—çŠ¶æ€ä»·å€¼
        actions_hidden_states: (B, C * A_dim, D), 
        """
        # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # actions_hidden_states çš„ç±»å‹éœ€è¦ä¸ attn_pool åŒ¹é…
        scores = self.attn_pool(actions_hidden_states)  # (B, num_tokens, 1)
        
        # 2. åº”ç”¨softmaxè·å–æ³¨æ„åŠ›æƒé‡
        weights = torch.softmax(scores, dim=1)  # (B, num_tokens, 1)
        
        # 3. åŠ æƒå¹³å‡å¾—åˆ°æ± åŒ–è¡¨ç¤º
        pooled = torch.sum(weights * actions_hidden_states, dim=1)  # (B, D)
        step_embedding  = self.step_count_emb(step_counts_batch)  # (B, D)
        pooled_step  = pooled + step_embedding   # (B, D)     

        # 4. é€šè¿‡ä»·å€¼å¤´è®¡ç®—æœ€ç»ˆä»·å€¼
        value = self.value_head(pooled_step).squeeze(-1)  # (B,)
        return value.to(torch.float32)

    def forward(self, inputs_batch: Dict[str, Any], return_vit_out=False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Returns:
          actions_all: (B, NUM_ACTIONS_CHUNK, ACTION_DIM)
          mu_all:      (B, NUM_ACTIONS_CHUNK, ACTION_DIM)
          log_std_all: (B, NUM_ACTIONS_CHUNK, ACTION_DIM)
          value:       (B,)
        """
        # Sanity checks
        for k in ("input_ids", "attention_mask", "pixel_values", "labels", "proprio"):
            if k not in inputs_batch:
                raise KeyError(f"inputs_batch missing key: {k}")

        # 1) VLA forward to obtain hidden states
        output = self._forward_vla(inputs_batch)
        last_hidden_states = output.hidden_states[-1]  # (B, seq_len, D)

        # 2) Predict continuous actions mean (mu) using action-related hidden states
        actions_hidden_states = self._extract_actions_hidden(last_hidden_states, inputs_batch['labels'], has_act_emb=("this_act_emb" in inputs_batch))
        predicted_actions = self.action_head.predict_action(actions_hidden_states)  # (B, NUM_ACTIONS_CHUNK, ACTION_DIM) or flat
        if predicted_actions.dim() == 3:
            mu_all = predicted_actions
        else:
            raise ValueError(f"Unexpected predicted_actions shape: {predicted_actions.shape}")

        # 3) Condition-independent log_std broadcast across chunks
        B = mu_all.size(0)
        log_std = self.log_std_param  # (NUM_ACTIONS_CHUNK, ACTION_DIM)
        log_std_all = log_std.unsqueeze(dim=0).expand(B, NUM_ACTIONS_CHUNK, ACTION_DIM)  # (B, T, A)

        # 5) Value from hidden states
        value = self._compute_value_from_hidden(actions_hidden_states.detach(), inputs_batch["step_count"])   # (B,)

        if return_vit_out:
            return mu_all.to(torch.float32), log_std_all.to(torch.float32), value.to(torch.float32), output.projector_features.to(torch.float32)
        else:
            return mu_all.to(torch.float32), log_std_all.to(torch.float32), value.to(torch.float32)

    def load_log_std(self, checkpoint_dir: str, step: int|str):
        # --- åŠ è½½ Log_Std parameter ---
        log_std_head_path = os.path.join(checkpoint_dir, f"log_std_head--{step}_checkpoint.pt")
        if not os.path.exists(log_std_head_path):
            raise FileNotFoundError(f"Log_Std Head checkpoint not found at: {log_std_head_path}")
        
        print(f"  -> Loading Log_std from {log_std_head_path}")
        loaded_data = torch.load(log_std_head_path, map_location=self.device)
        
        if isinstance(self.log_std_param, nn.Module):
            print("  -> Target `self.log_std_param` is an nn.Module. Attempting to load state_dict.")
            state_dict = loaded_data
            # å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒ (DDP) ä¿å­˜çš„ 'module.' å‰ç¼€
            if all(key.startswith('module.') for key in state_dict.keys()):
                print("  -> Removing 'module.' prefix from state_dict keys.")
                state_dict = {k.partition('module.')[2]: v for k, v in state_dict.items()}
            
            self.log_std_param.load_state_dict(state_dict)

        elif isinstance(self.log_std_param, nn.Parameter):
            print("  -> Target `self.log_std_param` is an nn.Parameter. Attempting to load data.")
            tensor_to_load = loaded_data['log_std_param']
            with torch.no_grad():
                self.log_std_param.data.copy_(tensor_to_load)
        
        else:
            # å¦‚æœ self.log_std_param ä¸æ˜¯æˆ‘ä»¬æ”¯æŒçš„ç±»å‹
             raise TypeError(f"self.log_std_param is of an unsupported type: {type(self.log_std_param)}")

        print("Log_std parameter loading complete.")

    def prepare_inputs_batch(self, inp, max_len=None):
        return prepare_inputs_batch(self, inp, max_len)

    def get_norm_stats(self):
        return self.vla.norm_stats[self.cfg.unnorm_key]["proprio"]

    def _compute_num_patches(self):
        return compute_num_patches(self.vla, self.cfg)


if __name__ == "__main__":
    import numpy as np
    import random
    import time
    from experiments.robot.robot_utils import set_seed_everywhere
    
    # Libero env wrapper and helpers
    from rl.libero_env import LiberoEnvWrapper
    from rl.utils import prepare_one_obs, check_unnorm_key
    from experiments.robot.libero.libero_utils import GenerateConfig, TaskSuite

    # Precision policy to match the example
    USE_BF16: bool = True
    TORCH_DTYPE = torch.bfloat16 if USE_BF16 else torch.float32

    # åœ¨è¿™é‡Œè®¾ç½®è¦å¹¶è¡Œå¤„ç†çš„ç¯å¢ƒæ•°é‡
    ENVS_ID = list(range(10))
    envs_num = len(ENVS_ID)
    BENCHMARK = TaskSuite.LIBERO_SPATIAL
    # LIBERO_SPATIAL: 93.2% (10 envs, 2982 episodes)
    # LIBERO_GOAL: 84.1% (10 envs, 2101 episodes)
    # LIBERO_OBJECT: 51.6% (10 envs, 1475 episodes)

    unnorm_key = f"{BENCHMARK}_no_noops"
    # Instantiate config
    cfg = GenerateConfig(
        pretrained_checkpoint="/cpfs01/liuwei_workspace/openvla_oft_rl/ckpt/finetune_nll_16/openvla-7b-oft-finetuned-libero-spatial-object-goal-10+libero_spatial_no_noops+b16+lr-0.0005+lora-r32+dropout-0.0--image_aug--parallel_dec--8_acts_chunk--continuous_acts--L1_regression--3rd_person_img--wrist_img--proprio_state", #/cpfs01/lcx_workspace/models/openvla-7b-oft-finetuned-libero-spatial-object-goal-10/
        use_l1_regression=True,
        use_diffusion=False,
        use_film=False,
        num_images_in_input=2,
        use_proprio=True,
        load_in_8bit=False,
        load_in_4bit=False,
        center_crop=True,
        num_open_loop_steps=NUM_ACTIONS_CHUNK,
        unnorm_key=unnorm_key,
        device=torch.device("cuda:3")
    )
    set_seed_everywhere(cfg.seed)
    # Create ActorCritic policy
    actor = ActorCritic(cfg, TORCH_DTYPE)
    # actor.load_log_std(cfg.pretrained_checkpoint, step="latest")

    check_unnorm_key(cfg, actor.vla)
    actor.get_parameter_groups()
    actor.eval()
    for key, value in actor.named_parameters():
        if value.dtype != TORCH_DTYPE:
            print(f"è­¦å‘Š: å‚æ•° {key} çš„æ•°æ®ç±»å‹æ˜¯ {value.dtype}, ä½†æœŸæœ›çš„æ˜¯ {TORCH_DTYPE}.")
    print("ç­–ç•¥åˆå§‹åŒ–å®Œæˆã€‚")

    # --- å¹¶è¡Œåˆå§‹åŒ–å¤šä¸ªç¯å¢ƒ ---
    print(f"æ­£åœ¨åˆå§‹åŒ– {len(ENVS_ID)} ä¸ªå¹¶è¡Œçš„ Libero ç¯å¢ƒ...")
    envs = [
        LiberoEnvWrapper(
            benchmark_name=BENCHMARK,
            task_id=env_id,  # æ¯ä¸ªç¯å¢ƒä¸€ä¸ªéšæœºä»»åŠ¡
            image_size=224,
            render_mode="rgb_array",
        )
        for env_id in ENVS_ID
    ]
    print("æ‰€æœ‰ç¯å¢ƒåˆå§‹åŒ–å®Œæˆã€‚")

    # --- åˆå§‹åŒ–æ‰€æœ‰ç¯å¢ƒçš„çŠ¶æ€ ---
    # ä½¿ç”¨åˆ—è¡¨æ¥ç‹¬ç«‹è·Ÿè¸ªæ¯ä¸ªç¯å¢ƒçš„çŠ¶æ€
    observations = []
    task_descriptions = []
    for i, env in enumerate(envs):
        # ä¸ºæ¯ä¸ªç¯å¢ƒè®¾ç½®ä¸åŒçš„éšæœºç§å­ä»¥ä¿è¯å¤šæ ·æ€§
        obs, info = env.reset(seed=int(time.time()) + i)
        observations.append(obs)
        task_descriptions.append(env.task_description)
        print(f"ç¯å¢ƒ {i}: ä»»åŠ¡ ID = {env.task_id}, ä»»åŠ¡æè¿° = {env.task_description}")

    # è·Ÿè¸ªæ¯ä¸ªç¯å¢ƒæ˜¯å¦ä»åœ¨æ´»åŠ¨ã€å¥–åŠ±å’Œæ­¥æ•°
    active_envs = [True] * envs_num
    total_rewards = [0.0] * envs_num
    episode_steps = [0] * envs_num
    success_info = [False] * envs_num

    # ç”¨äºç»Ÿè®¡æœ€ç»ˆæˆåŠŸç‡
    total_episodes_finished = 0
    total_successes = 0

    print("\nå¼€å§‹å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ç¯å¢ƒ...")

    # --- ä¸»å¾ªç¯ï¼šåªè¦æœ‰ä»»ä½•ä¸€ä¸ªç¯å¢ƒåœ¨æ´»åŠ¨ï¼Œå°±ç»§ç»­ ---
    while any(active_envs):
        # 1. ä»æ‰€æœ‰ã€æ´»åŠ¨ã€‘çš„ç¯å¢ƒä¸­æ”¶é›†è¾“å…¥æ•°æ®
        inputs_t_list = []
        # è®°å½•å½“å‰æ‰¹æ¬¡ä¸­æ•°æ®å¯¹åº”çš„åŸå§‹ç¯å¢ƒç´¢å¼•
        active_indices_this_step = []
        
        for i in range(envs_num):
            if active_envs[i]:
                inputs_t = prepare_one_obs(cfg, actor.processor, observations[i], task_descriptions[i], TORCH_DTYPE)
                inputs_t["step_count"] = torch.tensor([0], dtype=torch.long)
                inputs_t_list.append(inputs_t)
                active_indices_this_step.append(i)

        # å¦‚æœæ²¡æœ‰æ´»åŠ¨çš„è¾“å…¥ï¼Œåˆ™é€€å‡ºå¾ªç¯
        if not inputs_t_list:
            break

        # 2. ä½¿ç”¨ç±»æ–¹æ³•å°†è¾“å…¥åˆ—è¡¨æ‰¹å¤„ç†æˆä¸€ä¸ªå¤§çš„å¼ é‡
        #    è¿™æ˜¯å®ç°å¹¶è¡Œå¤„ç†çš„å…³é”®æ­¥éª¤
        inputs_batch = actor.prepare_inputs_batch(inputs_t_list)

        # 3. æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œä¸ºæ‰¹æ¬¡ä¸­çš„æ‰€æœ‰ç¯å¢ƒè·å–åŠ¨ä½œ
        with torch.no_grad():
            # actions_all çš„å½¢çŠ¶æ˜¯ (batch_size, num_chunks, action_dim)
            # å…¶ä¸­ batch_size ç­‰äºå½“å‰æ´»åŠ¨çš„ä»»åŠ¡æ•°é‡ len(inputs_t_list)
            mu_all, _, _ = actor.forward(inputs_batch)
            action_all = torch.clamp(mu_all, -1.0, 1.0)
            # action_all = torch.clamp(sample_all, -1.0, 1.0)

        # 4. å°†æ‰¹æ¬¡åŠ¨ä½œåˆ†å‘å›å„è‡ªçš„ç¯å¢ƒå¹¶æ‰§è¡Œä¸€æ­¥
        for i, env_idx in enumerate(active_indices_this_step):
            # i æ˜¯æ‰¹æ¬¡ä¸­çš„ç´¢å¼•, env_idx æ˜¯åŸå§‹ç¯å¢ƒåˆ—è¡¨ä¸­çš„ç´¢å¼•
            action_norm = action_all[i, 0].cpu().numpy().astype(np.float32)
            action_env = actor.vla._unnormalize_actions(action_norm, cfg.unnorm_key)

            # åœ¨å¯¹åº”çš„ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = envs[env_idx].step(action_env)

            # æ›´æ–°è¯¥ç¯å¢ƒçš„çŠ¶æ€
            observations[env_idx] = obs
            total_rewards[env_idx] += float(reward)
            episode_steps[env_idx] += 1

            # ä½¿ç”¨ç¡®å®šæ€§æ‰“å°
            if episode_steps[env_idx] % 50 == 0:
                print(f"ç¯å¢ƒ {env_idx}, Step: {episode_steps[env_idx]}, å¥–åŠ±: {reward:.4f}, ç»ˆæ­¢: {terminated}, æˆªæ–­: {truncated}")

            # 5. æ£€æŸ¥ç¯å¢ƒæ˜¯å¦å®Œæˆ
            if terminated or truncated:
                is_success = info.get('is_success', False)
                total_successes += is_success
                total_episodes_finished += 1
                success_info[env_idx] = is_success
                
                # æ‰“å°å•ä¸ªç¯å¢ƒå®Œæˆçš„ä¿¡æ¯
                print("-" * 40)
                print(f"ç¯å¢ƒ {env_idx} å·²å®Œæˆ (ä»»åŠ¡: {envs[env_idx].task_description[:50]}...)")
                print(f"  æ€»æ­¥æ•°: {episode_steps[env_idx]}, æ€»å¥–åŠ±: {total_rewards[env_idx]:.4f}, æ˜¯å¦æˆåŠŸ: {is_success}")
                print(f"Success rate: {total_successes / total_episodes_finished}, total_episodes_finished: {total_episodes_finished}")
                print("-" * 40)
                episode_steps[env_idx] = 0
                total_rewards[env_idx] = 0
                obs, info = envs[env_idx].reset(seed=random.randint(0, 1000))
                observations[env_idx] = obs
